{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DATA PROCESSING"
      ],
      "metadata": {
        "id": "0jl_uHm8Y0ye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XY8J6xOOMh",
        "outputId": "498b95f6-75b2-4954-9075-9f647f61919e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 561753746/561753746 [00:21<00:00, 25653100.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n",
            "train_X shape: torch.Size([112800, 28, 28])\n",
            "train_Y shape: torch.Size([112800])\n",
            "test_X shape: torch.Size([18800, 28, 28])\n",
            "test_Y shape: torch.Size([18800])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create instances of the DataLoader class\n",
        "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=64, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,shuffle=False,batch_size=64, num_workers=2)\n",
        "\n",
        "\n",
        "# Extract the input data and target labels\n",
        "X_train, y_train = train_data.data, train_data.targets\n",
        "X_test, y_test = test_data.data, test_data.targets\n",
        "# Print the shapes of the input data and target labels\n",
        "print('train_X shape:', X_train.shape)\n",
        "print('train_Y shape:', y_train.shape)\n",
        "print('test_X shape:', X_test.shape)\n",
        "print('test_Y shape:', y_test.shape)\n",
        "\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape)\n",
        "    print(targets.shape)\n",
        "    break  # to print only the first batch\n",
        "\n",
        "for input, target in test_loader:\n",
        "    print(input.shape)\n",
        "    print(target.shape)\n",
        "    break  # to print only the first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINE"
      ],
      "metadata": {
        "id": "wSl4Ub-VetZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Create instances of the DataLoader class\n",
        "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=64, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,shuffle=False,batch_size=64, num_workers=2)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "        from sklearn.model_selection import KFold\n",
        "\n",
        "# Define the train function\n",
        "def train(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # Move data to GPU\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler = torch.optim.lr_scheduler.stepLR(optimizer, step_size=5,gamma=0.1)\n",
        "\n",
        "# Define the evaluate function\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            # Move data to GPU\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Define the number of folds\n",
        "n_splits = 5\n",
        "\n",
        "# Create a list to store the validation accuracies for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Create the KFold object\n",
        "kf = KFold(n_splits=n_splits, shuffle=True)\n",
        "# Loop over the folds\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(train_data)):\n",
        "    # Get the training and validation data for this fold\n",
        "    train_subset = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_subset = torch.utils.data.Subset(train_data, val_indices)\n",
        "\n",
        "    # Create the data loaders for this fold\n",
        "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Create a new instance of the CNN model and move it to GPU\n",
        "    model = CNN().to(device)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Train the model for n_epochs epochs on this fold's training data\n",
        "    n_epochs = 5\n",
        "    for epoch in range(n_epochs):\n",
        "        train(model, train_loader, criterion, optimizer)\n",
        "\n",
        "    # Evaluate the model on this fold's validation data\n",
        "    accuracy = evaluate(model, val_loader)\n",
        "\n",
        "    # Append the validation accuracy to the list of accuracies\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    \n",
        "    # Print the current accuracy for this fold\n",
        "    print(f\"Fold {fold}: Validation accuracy = {accuracy}\")\n",
        "    \n",
        "# Calculate the average accuracy across all folds\n",
        "avg_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f\"Average accuracy across {n_splits} folds = {avg_accuracy}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e60f59a-0d58-4d75-ff04-8f9d888a42ce",
        "id": "istcqXwUvrRi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Fold 0: Validation accuracy = 86.875\n",
            "Fold 1: Validation accuracy = 87.46897163120568\n",
            "Fold 2: Validation accuracy = 86.98581560283688\n",
            "Fold 3: Validation accuracy = 86.72872340425532\n",
            "Fold 4: Validation accuracy = 87.15868794326241\n",
            "Average accuracy across 5 folds = 87.04343971631207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vj2Aag346_1",
        "outputId": "f1610f2c-03df-477a-8ddd-ae22d0ac16c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheduler: StepLR, Fold: 1, Validation Accuracy: 84.23%\n",
            "Scheduler: StepLR, Fold: 1, Validation Accuracy: 86.04%\n",
            "Scheduler: StepLR, Fold: 1, Validation Accuracy: 86.84%\n",
            "Scheduler: StepLR, Fold: 1, Validation Accuracy: 86.72%\n",
            "Scheduler: StepLR, Fold: 1, Validation Accuracy: 86.68%\n",
            "Scheduler: StepLR, Fold: 2, Validation Accuracy: 83.85%\n",
            "Scheduler: StepLR, Fold: 2, Validation Accuracy: 86.09%\n",
            "Scheduler: StepLR, Fold: 2, Validation Accuracy: 86.50%\n",
            "Scheduler: StepLR, Fold: 2, Validation Accuracy: 87.38%\n",
            "Scheduler: StepLR, Fold: 2, Validation Accuracy: 87.23%\n",
            "Scheduler: StepLR, Fold: 3, Validation Accuracy: 84.40%\n",
            "Scheduler: StepLR, Fold: 3, Validation Accuracy: 86.02%\n",
            "Scheduler: StepLR, Fold: 3, Validation Accuracy: 86.95%\n",
            "Scheduler: StepLR, Fold: 3, Validation Accuracy: 86.85%\n",
            "Scheduler: StepLR, Fold: 3, Validation Accuracy: 86.91%\n",
            "Scheduler: StepLR, Fold: 4, Validation Accuracy: 84.56%\n",
            "Scheduler: StepLR, Fold: 4, Validation Accuracy: 86.34%\n",
            "Scheduler: StepLR, Fold: 4, Validation Accuracy: 86.41%\n",
            "Scheduler: StepLR, Fold: 4, Validation Accuracy: 86.89%\n",
            "Scheduler: StepLR, Fold: 4, Validation Accuracy: 86.81%\n",
            "Scheduler: StepLR, Fold: 5, Validation Accuracy: 84.05%\n",
            "Scheduler: StepLR, Fold: 5, Validation Accuracy: 85.65%\n",
            "Scheduler: StepLR, Fold: 5, Validation Accuracy: 86.37%\n",
            "Scheduler: StepLR, Fold: 5, Validation Accuracy: 86.71%\n",
            "Scheduler: StepLR, Fold: 5, Validation Accuracy: 86.91%\n",
            "Scheduler: MultiStepLR, Fold: 1, Validation Accuracy: 83.71%\n",
            "Scheduler: MultiStepLR, Fold: 1, Validation Accuracy: 86.02%\n",
            "Scheduler: MultiStepLR, Fold: 1, Validation Accuracy: 86.44%\n",
            "Scheduler: MultiStepLR, Fold: 1, Validation Accuracy: 86.95%\n",
            "Scheduler: MultiStepLR, Fold: 1, Validation Accuracy: 86.79%\n",
            "Scheduler: MultiStepLR, Fold: 2, Validation Accuracy: 83.72%\n",
            "Scheduler: MultiStepLR, Fold: 2, Validation Accuracy: 86.25%\n",
            "Scheduler: MultiStepLR, Fold: 2, Validation Accuracy: 87.16%\n",
            "Scheduler: MultiStepLR, Fold: 2, Validation Accuracy: 86.46%\n",
            "Scheduler: MultiStepLR, Fold: 2, Validation Accuracy: 87.04%\n",
            "Scheduler: MultiStepLR, Fold: 3, Validation Accuracy: 84.39%\n",
            "Scheduler: MultiStepLR, Fold: 3, Validation Accuracy: 86.65%\n",
            "Scheduler: MultiStepLR, Fold: 3, Validation Accuracy: 86.72%\n",
            "Scheduler: MultiStepLR, Fold: 3, Validation Accuracy: 86.56%\n",
            "Scheduler: MultiStepLR, Fold: 3, Validation Accuracy: 86.54%\n",
            "Scheduler: MultiStepLR, Fold: 4, Validation Accuracy: 85.04%\n",
            "Scheduler: MultiStepLR, Fold: 4, Validation Accuracy: 86.37%\n",
            "Scheduler: MultiStepLR, Fold: 4, Validation Accuracy: 86.75%\n",
            "Scheduler: MultiStepLR, Fold: 4, Validation Accuracy: 86.76%\n",
            "Scheduler: MultiStepLR, Fold: 4, Validation Accuracy: 86.96%\n",
            "Scheduler: MultiStepLR, Fold: 5, Validation Accuracy: 84.11%\n",
            "Scheduler: MultiStepLR, Fold: 5, Validation Accuracy: 85.38%\n",
            "Scheduler: MultiStepLR, Fold: 5, Validation Accuracy: 86.81%\n",
            "Scheduler: MultiStepLR, Fold: 5, Validation Accuracy: 86.75%\n",
            "Scheduler: MultiStepLR, Fold: 5, Validation Accuracy: 86.69%\n",
            "Scheduler: CosineAnnealingLR, Fold: 1, Validation Accuracy: 84.05%\n",
            "Scheduler: CosineAnnealingLR, Fold: 1, Validation Accuracy: 86.68%\n",
            "Scheduler: CosineAnnealingLR, Fold: 1, Validation Accuracy: 85.85%\n",
            "Scheduler: CosineAnnealingLR, Fold: 1, Validation Accuracy: 87.47%\n",
            "Scheduler: CosineAnnealingLR, Fold: 1, Validation Accuracy: 87.44%\n",
            "Scheduler: CosineAnnealingLR, Fold: 2, Validation Accuracy: 85.22%\n",
            "Scheduler: CosineAnnealingLR, Fold: 2, Validation Accuracy: 86.40%\n",
            "Scheduler: CosineAnnealingLR, Fold: 2, Validation Accuracy: 86.34%\n",
            "Scheduler: CosineAnnealingLR, Fold: 2, Validation Accuracy: 86.94%\n",
            "Scheduler: CosineAnnealingLR, Fold: 2, Validation Accuracy: 87.45%\n",
            "Scheduler: CosineAnnealingLR, Fold: 3, Validation Accuracy: 84.68%\n",
            "Scheduler: CosineAnnealingLR, Fold: 3, Validation Accuracy: 85.97%\n",
            "Scheduler: CosineAnnealingLR, Fold: 3, Validation Accuracy: 86.69%\n",
            "Scheduler: CosineAnnealingLR, Fold: 3, Validation Accuracy: 87.23%\n",
            "Scheduler: CosineAnnealingLR, Fold: 3, Validation Accuracy: 87.05%\n",
            "Scheduler: CosineAnnealingLR, Fold: 4, Validation Accuracy: 84.78%\n",
            "Scheduler: CosineAnnealingLR, Fold: 4, Validation Accuracy: 85.78%\n",
            "Scheduler: CosineAnnealingLR, Fold: 4, Validation Accuracy: 86.85%\n",
            "Scheduler: CosineAnnealingLR, Fold: 4, Validation Accuracy: 86.72%\n",
            "Scheduler: CosineAnnealingLR, Fold: 4, Validation Accuracy: 87.00%\n",
            "Scheduler: CosineAnnealingLR, Fold: 5, Validation Accuracy: 84.10%\n",
            "Scheduler: CosineAnnealingLR, Fold: 5, Validation Accuracy: 85.75%\n",
            "Scheduler: CosineAnnealingLR, Fold: 5, Validation Accuracy: 86.15%\n",
            "Scheduler: CosineAnnealingLR, Fold: 5, Validation Accuracy: 86.11%\n",
            "Scheduler: CosineAnnealingLR, Fold: 5, Validation Accuracy: 86.74%\n",
            "Average validation accuracy for StepLR: 86.14%\n",
            "Average validation accuracy for MultiStepLR: 86.12%\n",
            "Average validation accuracy for CosineAnnealingLR: 86.22%\n",
            "The best scheduler is CosineAnnealingLR with an average validation accuracy of 86.22%\n"
          ]
        }
      ],
      "source": [
        "#learning rate scheduler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Define the number of folds\n",
        "k = 5\n",
        "\n",
        "# Create a KFold object\n",
        "kf = KFold(n_splits=k)\n",
        "\n",
        "# Define a list to store the validation accuracies for each fold\n",
        "val_accuracies = []\n",
        "\n",
        "# Define the learning rate scheduling methods\n",
        "schedulers = [\n",
        "    (\"StepLR\", torch.optim.lr_scheduler.StepLR, {\"step_size\": 5, \"gamma\": 0.1}),\n",
        "    (\"MultiStepLR\", torch.optim.lr_scheduler.MultiStepLR, {\"milestones\": [5, 10, 15], \"gamma\": 0.1}),\n",
        "    (\"CosineAnnealingLR\", torch.optim.lr_scheduler.CosineAnnealingLR, {\"T_max\": 5, \"eta_min\": 0.001})\n",
        "]\n",
        "\n",
        "# Define a dictionary to store the average validation accuracies for each scheduler\n",
        "scheduler_avg_val_accuracies = {}\n",
        "\n",
        "# Iterate over the learning rate scheduling methods\n",
        "for name, scheduler_cls, params in schedulers:\n",
        "    # Define a list to store the validation accuracies for each fold\n",
        "    fold_val_accuracies = []\n",
        "\n",
        "    # Iterate over the folds\n",
        "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_data)):\n",
        "        # Split the data into training and validation sets for this fold\n",
        "        train_dataset = torch.utils.data.Subset(train_data, train_indices)\n",
        "        val_dataset = torch.utils.data.Subset(train_data, val_indices)\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # Define the model, optimizer, and loss function\n",
        "        model = CNN()\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train the model on the training set for this fold\n",
        "        val_accuracies = []\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                data, target = data, target\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Evaluate the model on the validation set for this fold\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for data, target in val_loader:\n",
        "                    data, target = data, target\n",
        "                    outputs = model(data)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += target.size(0)\n",
        "                    correct += (predicted == target).sum().item()\n",
        "\n",
        "                accuracy = 100 * correct / total\n",
        "                print(f\"Scheduler: {name}, Fold: {fold+1}, Validation Accuracy: {accuracy:.2f}%\", flush=True)\n",
        "            # Store the validation accuracy for this fold\n",
        "            val_accuracies.append(accuracy)\n",
        "             \n",
        "        # Store the average validation accuracy for this fold\n",
        "        fold_val_accuracies.append(sum(val_accuracies) / len(val_accuracies))\n",
        "\n",
        "    # Store the average validation accuracies for this scheduler\n",
        "    scheduler_avg_val_accuracies[name] = sum(fold_val_accuracies) / len(fold_val_accuracies)\n",
        "\n",
        "# Print the average validation accuracies for each scheduler\n",
        "for name, avg_val_accuracy in scheduler_avg_val_accuracies.items():\n",
        "    print(f\"Average validation accuracy for {name}: {avg_val_accuracy:.2f}%\")\n",
        "\n",
        "# Print the best scheduler based on the highest average validation accuracy\n",
        "best_scheduler = max(scheduler_avg_val_accuracies, key=scheduler_avg_val_accuracies.get)\n",
        "print(f\"The best scheduler is {best_scheduler} with an average validation accuracy of {scheduler_avg_val_accuracies[best_scheduler]:.2f}%\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "activation_functions"
      ],
      "metadata": {
        "id": "8-eCTCshZUGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import math\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation_fn):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.activation1 = activation_fn\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.activation2 = activation_fn\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.activation1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.activation2(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Define the number of folds\n",
        "k = 5\n",
        "# Define the batch size, number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4\n",
        "# Define some constants\n",
        "num_iters_per_epoch = 100\n",
        "\n",
        "\n",
        "# Define the activation functions to use\n",
        "activation_functions = [\n",
        "    (\"ReLU\", nn.ReLU()),\n",
        "    (\"LeakyReLU\", nn.LeakyReLU()),\n",
        "    (\"ELU\", nn.ELU())\n",
        "]\n",
        "# Choose an activation function\n",
        "activation_fn = nn.ReLU()\n",
        "\n",
        "# Create the CNN model with the chosen activation function\n",
        "model = CNN(activation_fn)\n",
        "\n",
        "# Initialize variables to store the average validation accuracy for each activation function\n",
        "avg_val_accs = {}\n",
        "for activation_fn_name, _ in activation_functions:\n",
        "    avg_val_accs[activation_fn_name] = 0\n",
        "\n",
        "# Define a dictionary to store the average validation accuracy for each activation function\n",
        "avg_val_acc = {activation_fn_name: 0.0 for activation_fn_name, _ in activation_functions}\n",
        "\n",
        "# Define the overall best activation function and its average validation accuracy\n",
        "best_activation_fn_name = \"\"\n",
        "best_avg_val_acc = 0.0\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "def cosine_scheduler(optimizer, lr_init, num_epochs, num_iters_per_epoch):\n",
        "    def _scheduler(epoch):\n",
        "        return lr_init * 0.5 * (1 + math.cos(epoch * math.pi / num_epochs))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, _scheduler)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "# Move the model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# Iterate over the folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(train_data)):\n",
        "    # Split the data into training and validation sets for this fold\n",
        "    train_dataset = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(train_data, val_indices)\n",
        "\n",
        "    # Define the data loaders for training and validation\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    # Define the optimizer and the learning rate scheduler\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = cosine_scheduler(optimizer,lr, num_epochs, num_iters_per_epoch) \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Move the data to GPU\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Adjust the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                # Move the data to GPU\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, target).item()\n",
        "\n",
        "                # Compute the number of correct predictions\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        # Compute the average validation loss and accuracy\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # Print the results for this epoch\n",
        "        print(\"Fold: {} Epoch: {} Val Loss: {:.6f} Val Acc: {:.2f}%\".format(fold, epoch+1, avg_val_loss, avg_val_acc*100))\n",
        "\n",
        "        # Update the best activation function and its average validation accuracy\n",
        "        if avg_val_acc > best_avg_val_acc:\n",
        "            best_avg_val_acc = avg_val_acc\n",
        "            best_activation_fn_name = activation_fn.__class__.__name__\n",
        "\n",
        "# Print the overall best activation function and its average validation accuracy\n",
        "print(\"Best Activation Function: {} Best Avg Val Acc: {:.2f}%\".format(best_activation_fn_name, best_avg_val_acc*100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "190QLKaMLsAB",
        "outputId": "2c4d2639-d9bf-40bf-9583-a74de3441e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Fold: 0 Epoch: 1 Val Loss: 1.134477 Val Acc: 70.51%\n",
            "Fold: 0 Epoch: 2 Val Loss: 0.862729 Val Acc: 76.06%\n",
            "Fold: 0 Epoch: 3 Val Loss: 0.770252 Val Acc: 78.12%\n",
            "Fold: 0 Epoch: 4 Val Loss: 0.734854 Val Acc: 78.85%\n",
            "Fold: 0 Epoch: 5 Val Loss: 0.725539 Val Acc: 79.24%\n",
            "Fold: 1 Epoch: 1 Val Loss: 0.645622 Val Acc: 81.04%\n",
            "Fold: 1 Epoch: 2 Val Loss: 0.608832 Val Acc: 81.74%\n",
            "Fold: 1 Epoch: 3 Val Loss: 0.584269 Val Acc: 82.55%\n",
            "Fold: 1 Epoch: 4 Val Loss: 0.575683 Val Acc: 82.70%\n",
            "Fold: 1 Epoch: 5 Val Loss: 0.571325 Val Acc: 82.82%\n",
            "Fold: 2 Epoch: 1 Val Loss: 0.553881 Val Acc: 83.28%\n",
            "Fold: 2 Epoch: 2 Val Loss: 0.536137 Val Acc: 83.69%\n",
            "Fold: 2 Epoch: 3 Val Loss: 0.527515 Val Acc: 83.86%\n",
            "Fold: 2 Epoch: 4 Val Loss: 0.521420 Val Acc: 84.10%\n",
            "Fold: 2 Epoch: 5 Val Loss: 0.520386 Val Acc: 84.08%\n",
            "Fold: 3 Epoch: 1 Val Loss: 0.493941 Val Acc: 84.86%\n",
            "Fold: 3 Epoch: 2 Val Loss: 0.483978 Val Acc: 84.95%\n",
            "Fold: 3 Epoch: 3 Val Loss: 0.479181 Val Acc: 85.07%\n",
            "Fold: 3 Epoch: 4 Val Loss: 0.476786 Val Acc: 85.12%\n",
            "Fold: 3 Epoch: 5 Val Loss: 0.474479 Val Acc: 85.14%\n",
            "Fold: 4 Epoch: 1 Val Loss: 0.462522 Val Acc: 85.83%\n",
            "Fold: 4 Epoch: 2 Val Loss: 0.456763 Val Acc: 85.79%\n",
            "Fold: 4 Epoch: 3 Val Loss: 0.454843 Val Acc: 85.89%\n",
            "Fold: 4 Epoch: 4 Val Loss: 0.451114 Val Acc: 86.01%\n",
            "Fold: 4 Epoch: 5 Val Loss: 0.450645 Val Acc: 86.02%\n",
            "Best Activation Function: ReLU Best Avg Val Acc: 86.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGMGWh4OObWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizers"
      ],
      "metadata": {
        "id": "mlD9cr62Zaz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Create instances of the DataLoader class\n",
        "train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=64, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,shuffle=False,batch_size=64, num_workers=2)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "optimizers = {\n",
        "    'SGD': torch.optim.SGD,\n",
        "    'ADAM': torch.optim.Adam,\n",
        "    'RMSprop': torch.optim.RMSprop,\n",
        "    'ASGD': torch.optim.ASGD\n",
        "}\n",
        "\n",
        "# Set the hyperparameters\n",
        "lr = 0.001\n",
        "k = 5\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Create a dictionary to store the validation accuracy for each optimizer, fold, and epoch\n",
        "val_acc_dict = {}\n",
        "for optimizer_name in optimizers:\n",
        "    val_acc_dict[optimizer_name] = []\n",
        "\n",
        "# Apply k-fold cross-validation for each optimizer\n",
        "for optimizer_name, optimizer_fn in optimizers.items():\n",
        "    print(f'Optimizer: {optimizer_name}')\n",
        "    for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=k, shuffle=True, random_state=42).split(train_data)):\n",
        "        print(f'Fold [{fold+1}/{k}]')\n",
        "\n",
        "        # Initialize the model, optimizer, and scheduler for the current fold\n",
        "        model = CNN().to(device)\n",
        "        optimizer = optimizer_fn(model.parameters(), lr=lr)\n",
        "        \n",
        "        num_batches = len(train_loader) * num_epochs\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_batches)\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_data, batch_size=64, sampler=torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        )\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            train_data, batch_size=64, sampler=torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train the model for the specified number of epochs\n",
        "               # Train the model for the specified number of epochs\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = 0\n",
        "            model.train()\n",
        "            for images, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(images.to(device))\n",
        "                loss = criterion(output, labels.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * images.size(0)\n",
        "            train_loss /= len(train_loader.dataset)\n",
        "\n",
        "            # Compute the validation loss and accuracy\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "                val_loss /= len(val_loader.sampler)\n",
        "                val_acc = val_correct / len(val_loader.sampler)\n",
        "                val_acc_dict[optimizer_name].append(val_acc) # store validation accuracy for the current fold and epoch\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "            # Update the learning rate using MultiStepLR\n",
        "            scheduler.step()\n",
        "\n",
        "# Compute the average validation accuracy for each optimizer\n",
        "for optimizer_name in optimizers:\n",
        "    val_acc = sum(val_acc_dict[optimizer_name]) / (k * num_epochs)\n",
        "    print(f'Optimizer: {optimizer_name}, Average Validation Accuracy: {val_acc*100:.2f}%')\n",
        "\n",
        "optimizer_val_acc = {}\n",
        "for optimizer_name in optimizers:\n",
        "    val_acc = sum(val_acc_dict[optimizer_name]) / (k * num_epochs)\n",
        "    optimizer_val_acc[optimizer_name] = val_acc\n",
        "\n",
        "# Print the best optimizer\n",
        "best_optimizer = max(optimizer_val_acc, key=optimizer_val_acc.get)\n",
        "print(f'Best optimizer: {best_optimizer}, Average Validation Accuracy: {optimizer_val_acc[best_optimizer]*100:.2f}%')\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "       \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVkmBq87YKCi",
        "outputId": "afa0c341-5103-4936-cab8-4355dbd3d6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Optimizer: SGD\n",
            "Fold [1/5]\n",
            "Epoch [1/5], Train Loss: 1.5234, Validation Loss: 1.1647, Validation Accuracy: 0.7006\n",
            "Epoch [2/5], Train Loss: 0.7909, Validation Loss: 0.8727, Validation Accuracy: 0.7567\n",
            "Epoch [3/5], Train Loss: 0.6351, Validation Loss: 0.7401, Validation Accuracy: 0.7851\n",
            "Epoch [4/5], Train Loss: 0.5562, Validation Loss: 0.6685, Validation Accuracy: 0.8033\n",
            "Epoch [5/5], Train Loss: 0.5080, Validation Loss: 0.6232, Validation Accuracy: 0.8151\n",
            "Fold [2/5]\n",
            "Epoch [1/5], Train Loss: 1.4826, Validation Loss: 1.1362, Validation Accuracy: 0.7054\n",
            "Epoch [2/5], Train Loss: 0.7726, Validation Loss: 0.8397, Validation Accuracy: 0.7668\n",
            "Epoch [3/5], Train Loss: 0.6187, Validation Loss: 0.7180, Validation Accuracy: 0.7920\n",
            "Epoch [4/5], Train Loss: 0.5432, Validation Loss: 0.6473, Validation Accuracy: 0.8096\n",
            "Epoch [5/5], Train Loss: 0.4972, Validation Loss: 0.6046, Validation Accuracy: 0.8195\n",
            "Fold [3/5]\n",
            "Epoch [1/5], Train Loss: 1.5055, Validation Loss: 1.1317, Validation Accuracy: 0.7060\n",
            "Epoch [2/5], Train Loss: 0.7651, Validation Loss: 0.8436, Validation Accuracy: 0.7639\n",
            "Epoch [3/5], Train Loss: 0.6143, Validation Loss: 0.7260, Validation Accuracy: 0.7890\n",
            "Epoch [4/5], Train Loss: 0.5408, Validation Loss: 0.6589, Validation Accuracy: 0.8049\n",
            "Epoch [5/5], Train Loss: 0.4958, Validation Loss: 0.6186, Validation Accuracy: 0.8128\n",
            "Fold [4/5]\n",
            "Epoch [1/5], Train Loss: 1.4968, Validation Loss: 1.1466, Validation Accuracy: 0.6977\n",
            "Epoch [2/5], Train Loss: 0.7781, Validation Loss: 0.8514, Validation Accuracy: 0.7619\n",
            "Epoch [3/5], Train Loss: 0.6223, Validation Loss: 0.7252, Validation Accuracy: 0.7926\n",
            "Epoch [4/5], Train Loss: 0.5454, Validation Loss: 0.6539, Validation Accuracy: 0.8093\n",
            "Epoch [5/5], Train Loss: 0.4985, Validation Loss: 0.6108, Validation Accuracy: 0.8189\n",
            "Fold [5/5]\n",
            "Epoch [1/5], Train Loss: 1.4736, Validation Loss: 1.1226, Validation Accuracy: 0.7119\n",
            "Epoch [2/5], Train Loss: 0.7573, Validation Loss: 0.8446, Validation Accuracy: 0.7672\n",
            "Epoch [3/5], Train Loss: 0.6102, Validation Loss: 0.7269, Validation Accuracy: 0.7918\n",
            "Epoch [4/5], Train Loss: 0.5380, Validation Loss: 0.6595, Validation Accuracy: 0.8078\n",
            "Epoch [5/5], Train Loss: 0.4938, Validation Loss: 0.6154, Validation Accuracy: 0.8166\n",
            "Optimizer: ADAM\n",
            "Fold [1/5]\n",
            "Epoch [1/5], Train Loss: 0.5467, Validation Loss: 0.4871, Validation Accuracy: 0.8385\n",
            "Epoch [2/5], Train Loss: 0.3407, Validation Loss: 0.4490, Validation Accuracy: 0.8473\n",
            "Epoch [3/5], Train Loss: 0.2915, Validation Loss: 0.4016, Validation Accuracy: 0.8645\n",
            "Epoch [4/5], Train Loss: 0.2618, Validation Loss: 0.3816, Validation Accuracy: 0.8710\n",
            "Epoch [5/5], Train Loss: 0.2365, Validation Loss: 0.3897, Validation Accuracy: 0.8686\n",
            "Fold [2/5]\n",
            "Epoch [1/5], Train Loss: 0.5343, Validation Loss: 0.4796, Validation Accuracy: 0.8403\n",
            "Epoch [2/5], Train Loss: 0.3311, Validation Loss: 0.4162, Validation Accuracy: 0.8603\n",
            "Epoch [3/5], Train Loss: 0.2860, Validation Loss: 0.3973, Validation Accuracy: 0.8674\n",
            "Epoch [4/5], Train Loss: 0.2564, Validation Loss: 0.3860, Validation Accuracy: 0.8676\n",
            "Epoch [5/5], Train Loss: 0.2309, Validation Loss: 0.3836, Validation Accuracy: 0.8668\n",
            "Fold [3/5]\n",
            "Epoch [1/5], Train Loss: 0.5391, Validation Loss: 0.4712, Validation Accuracy: 0.8411\n",
            "Epoch [2/5], Train Loss: 0.3382, Validation Loss: 0.4410, Validation Accuracy: 0.8510\n",
            "Epoch [3/5], Train Loss: 0.2888, Validation Loss: 0.4087, Validation Accuracy: 0.8614\n",
            "Epoch [4/5], Train Loss: 0.2579, Validation Loss: 0.3834, Validation Accuracy: 0.8692\n",
            "Epoch [5/5], Train Loss: 0.2334, Validation Loss: 0.3818, Validation Accuracy: 0.8699\n",
            "Fold [4/5]\n",
            "Epoch [1/5], Train Loss: 0.5375, Validation Loss: 0.4845, Validation Accuracy: 0.8343\n",
            "Epoch [2/5], Train Loss: 0.3353, Validation Loss: 0.4205, Validation Accuracy: 0.8620\n",
            "Epoch [3/5], Train Loss: 0.2892, Validation Loss: 0.3918, Validation Accuracy: 0.8703\n",
            "Epoch [4/5], Train Loss: 0.2570, Validation Loss: 0.4062, Validation Accuracy: 0.8647\n",
            "Epoch [5/5], Train Loss: 0.2321, Validation Loss: 0.3771, Validation Accuracy: 0.8742\n",
            "Fold [5/5]\n",
            "Epoch [1/5], Train Loss: 0.5330, Validation Loss: 0.4961, Validation Accuracy: 0.8373\n",
            "Epoch [2/5], Train Loss: 0.3354, Validation Loss: 0.4257, Validation Accuracy: 0.8582\n",
            "Epoch [3/5], Train Loss: 0.2866, Validation Loss: 0.3981, Validation Accuracy: 0.8667\n",
            "Epoch [4/5], Train Loss: 0.2564, Validation Loss: 0.4041, Validation Accuracy: 0.8631\n",
            "Epoch [5/5], Train Loss: 0.2315, Validation Loss: 0.3866, Validation Accuracy: 0.8687\n",
            "Optimizer: RMSprop\n",
            "Fold [1/5]\n",
            "Epoch [1/5], Train Loss: 0.6686, Validation Loss: 0.4967, Validation Accuracy: 0.8371\n",
            "Epoch [2/5], Train Loss: 0.3390, Validation Loss: 0.3920, Validation Accuracy: 0.8669\n",
            "Epoch [3/5], Train Loss: 0.2832, Validation Loss: 0.3755, Validation Accuracy: 0.8704\n",
            "Epoch [4/5], Train Loss: 0.2491, Validation Loss: 0.3694, Validation Accuracy: 0.8713\n",
            "Epoch [5/5], Train Loss: 0.2258, Validation Loss: 0.3595, Validation Accuracy: 0.8728\n",
            "Fold [2/5]\n",
            "Epoch [1/5], Train Loss: 0.6871, Validation Loss: 0.5064, Validation Accuracy: 0.8292\n",
            "Epoch [2/5], Train Loss: 0.3402, Validation Loss: 0.4034, Validation Accuracy: 0.8618\n",
            "Epoch [3/5], Train Loss: 0.2834, Validation Loss: 0.3869, Validation Accuracy: 0.8691\n",
            "Epoch [4/5], Train Loss: 0.2521, Validation Loss: 0.3766, Validation Accuracy: 0.8722\n",
            "Epoch [5/5], Train Loss: 0.2282, Validation Loss: 0.3695, Validation Accuracy: 0.8748\n",
            "Fold [3/5]\n",
            "Epoch [1/5], Train Loss: 0.6854, Validation Loss: 0.5131, Validation Accuracy: 0.8309\n",
            "Epoch [2/5], Train Loss: 0.3383, Validation Loss: 0.4106, Validation Accuracy: 0.8634\n",
            "Epoch [3/5], Train Loss: 0.2847, Validation Loss: 0.4077, Validation Accuracy: 0.8632\n",
            "Epoch [4/5], Train Loss: 0.2510, Validation Loss: 0.3818, Validation Accuracy: 0.8713\n",
            "Epoch [5/5], Train Loss: 0.2273, Validation Loss: 0.3774, Validation Accuracy: 0.8719\n",
            "Fold [4/5]\n",
            "Epoch [1/5], Train Loss: 0.6867, Validation Loss: 0.4598, Validation Accuracy: 0.8460\n",
            "Epoch [2/5], Train Loss: 0.3385, Validation Loss: 0.4312, Validation Accuracy: 0.8544\n",
            "Epoch [3/5], Train Loss: 0.2856, Validation Loss: 0.3967, Validation Accuracy: 0.8660\n",
            "Epoch [4/5], Train Loss: 0.2525, Validation Loss: 0.3849, Validation Accuracy: 0.8741\n",
            "Epoch [5/5], Train Loss: 0.2298, Validation Loss: 0.3807, Validation Accuracy: 0.8711\n",
            "Fold [5/5]\n",
            "Epoch [1/5], Train Loss: 0.6935, Validation Loss: 0.4796, Validation Accuracy: 0.8395\n",
            "Epoch [2/5], Train Loss: 0.3442, Validation Loss: 0.4176, Validation Accuracy: 0.8596\n",
            "Epoch [3/5], Train Loss: 0.2883, Validation Loss: 0.3852, Validation Accuracy: 0.8689\n",
            "Epoch [4/5], Train Loss: 0.2559, Validation Loss: 0.3744, Validation Accuracy: 0.8727\n",
            "Epoch [5/5], Train Loss: 0.2311, Validation Loss: 0.3720, Validation Accuracy: 0.8719\n",
            "Optimizer: ASGD\n",
            "Fold [1/5]\n",
            "Epoch [1/5], Train Loss: 1.4937, Validation Loss: 1.1489, Validation Accuracy: 0.7009\n",
            "Epoch [2/5], Train Loss: 0.7841, Validation Loss: 0.8683, Validation Accuracy: 0.7564\n",
            "Epoch [3/5], Train Loss: 0.6348, Validation Loss: 0.7453, Validation Accuracy: 0.7848\n",
            "Epoch [4/5], Train Loss: 0.5581, Validation Loss: 0.6742, Validation Accuracy: 0.8023\n",
            "Epoch [5/5], Train Loss: 0.5103, Validation Loss: 0.6264, Validation Accuracy: 0.8147\n",
            "Fold [2/5]\n",
            "Epoch [1/5], Train Loss: 1.5106, Validation Loss: 1.1436, Validation Accuracy: 0.7039\n",
            "Epoch [2/5], Train Loss: 0.7766, Validation Loss: 0.8481, Validation Accuracy: 0.7617\n",
            "Epoch [3/5], Train Loss: 0.6227, Validation Loss: 0.7270, Validation Accuracy: 0.7912\n",
            "Epoch [4/5], Train Loss: 0.5470, Validation Loss: 0.6558, Validation Accuracy: 0.8066\n",
            "Epoch [5/5], Train Loss: 0.5003, Validation Loss: 0.6089, Validation Accuracy: 0.8195\n",
            "Fold [3/5]\n",
            "Epoch [1/5], Train Loss: 1.5363, Validation Loss: 1.1900, Validation Accuracy: 0.6912\n",
            "Epoch [2/5], Train Loss: 0.8054, Validation Loss: 0.8825, Validation Accuracy: 0.7552\n",
            "Epoch [3/5], Train Loss: 0.6415, Validation Loss: 0.7529, Validation Accuracy: 0.7830\n",
            "Epoch [4/5], Train Loss: 0.5587, Validation Loss: 0.6777, Validation Accuracy: 0.8013\n",
            "Epoch [5/5], Train Loss: 0.5082, Validation Loss: 0.6294, Validation Accuracy: 0.8115\n",
            "Fold [4/5]\n",
            "Epoch [1/5], Train Loss: 1.4571, Validation Loss: 1.1373, Validation Accuracy: 0.7012\n",
            "Epoch [2/5], Train Loss: 0.7786, Validation Loss: 0.8612, Validation Accuracy: 0.7584\n",
            "Epoch [3/5], Train Loss: 0.6302, Validation Loss: 0.7375, Validation Accuracy: 0.7881\n",
            "Epoch [4/5], Train Loss: 0.5544, Validation Loss: 0.6657, Validation Accuracy: 0.8046\n",
            "Epoch [5/5], Train Loss: 0.5068, Validation Loss: 0.6234, Validation Accuracy: 0.8164\n",
            "Fold [5/5]\n",
            "Epoch [1/5], Train Loss: 1.5108, Validation Loss: 1.1602, Validation Accuracy: 0.7011\n",
            "Epoch [2/5], Train Loss: 0.7816, Validation Loss: 0.8721, Validation Accuracy: 0.7586\n",
            "Epoch [3/5], Train Loss: 0.6290, Validation Loss: 0.7439, Validation Accuracy: 0.7869\n",
            "Epoch [4/5], Train Loss: 0.5523, Validation Loss: 0.6777, Validation Accuracy: 0.8020\n",
            "Epoch [5/5], Train Loss: 0.5049, Validation Loss: 0.6291, Validation Accuracy: 0.8153\n",
            "Optimizer: SGD, Average Validation Accuracy: 77.63%\n",
            "Optimizer: ADAM, Average Validation Accuracy: 85.94%\n",
            "Optimizer: RMSprop, Average Validation Accuracy: 86.20%\n",
            "Optimizer: ASGD, Average Validation Accuracy: 77.27%\n",
            "Best optimizer: RMSprop, Average Validation Accuracy: 86.20%\n",
            "Execution time: 3345.66 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITH BATCH NORMALIZE"
      ],
      "metadata": {
        "id": "GYXpqM7PZmkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##WITH BATCH NORMALIZE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "# # Create instances of the DataLoader class\n",
        "# train_loader = torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=64, num_workers=2)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data,shuffle=False,batch_size=64, num_workers=2)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 32\n",
        "lr = 0.01\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Training with RMSprop optimizer and CosineAnnealingLR learning rate scheduler\")\n",
        "# Define the data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Apply k-fold cross-validation\n",
        "accuracies = []\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
        "    print(f\"Fold [{fold+1}/{kf.n_splits}]\")\n",
        "   \n",
        "    # Initialize the model, optimizer, and scheduler for the current fold\n",
        "    model = CNN().to(device)\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    num_batches = len(train_loader) * num_epochs\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_batches)\n",
        "\n",
        "    # Define the data loaders for the current fold\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(train_data, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Set the optimizer to RMSprop and the learning rate scheduler to cosine annealing\n",
        "\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "num_batches = len(train_loader) * num_epochs\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_batches)\n",
        "\n",
        "# Train the model for the current fold\n",
        "for epoch in range(num_epochs):\n",
        "    # Train for one epoch\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    # Validate the model for one epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "            \n",
        "        # Compute the accuracy for the current fold\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        accuracy = 100. * correct / total\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Validation Loss: {val_loss:.4f} | Accuracy WitH batch normalization: {accuracy:.2f}%\")\n",
        "    # Calculate the accuracy for the current fold and add it to the list of accuracies\n",
        "    fold_acc = 100 * correct / total\n",
        "    accuracies.append(fold_acc)\n",
        "\n",
        "# Calculate the average accuracy over all folds\n",
        "avg_acc = sum(accuracies) / len(accuracies)\n",
        "print(f\"Average Accuracy: {avg_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kABjWdgYOlbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c7c7ba-b624-45a5-f652-c02438a132ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with RMSprop optimizer and CosineAnnealingLR learning rate scheduler\n",
            "Fold [1/5]\n",
            "Fold [2/5]\n",
            "Fold [3/5]\n",
            "Fold [4/5]\n",
            "Fold [5/5]\n",
            "Epoch [1/5] | Validation Loss: 0.0926 | Accuracy WitH batch normalization: 84.71%\n",
            "Epoch [2/5] | Validation Loss: 0.0811 | Accuracy WitH batch normalization: 86.35%\n",
            "Epoch [3/5] | Validation Loss: 0.0754 | Accuracy WitH batch normalization: 87.13%\n",
            "Epoch [4/5] | Validation Loss: 0.0730 | Accuracy WitH batch normalization: 87.55%\n",
            "Epoch [5/5] | Validation Loss: 0.0735 | Accuracy WitH batch normalization: 87.81%\n",
            "Average Accuracy: 86.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "regularization"
      ],
      "metadata": {
        "id": "77tPEH_5ZxQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 32\n",
        "lr = 0.01\n",
        "num_epochs = 5\n",
        "# Set learning rate and momentum\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the cross-validation folds\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Define dictionaries to store the average accuracy for each regularization method\n",
        "none_acc = []\n",
        "l1_acc = []\n",
        "l2_acc = []\n",
        "regularization_methods = {\n",
        "    'none': None,\n",
        "    'l1': 0.0001,\n",
        "    'l2': 0.0001\n",
        "}\n",
        "\n",
        "# Loop over the regularization methods\n",
        "for reg_name, reg_value in regularization_methods.items():\n",
        "    print('Using regularization method:', reg_name)\n",
        "\n",
        "    # Define lists to store accuracy for each fold\n",
        "    fold_none_acc = []\n",
        "    fold_l1_acc = []\n",
        "    fold_l2_acc = []\n",
        "\n",
        "    # Loop over the cross-validation folds\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(kf.split(train_data)):\n",
        "        print('Using fold', fold_idx)\n",
        "\n",
        "        # Create data loaders for training and validation sets\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "        # Move model and data to device\n",
        "        model.to(device)\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "        num_batches = len(train_loader) * num_epochs\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_batches)\n",
        "  \n",
        "        # Define optimizer with or without regularization\n",
        "        if reg_value is None:\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        elif reg_name == 'l1':\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=reg_value)\n",
        "        elif reg_name == 'l2':\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=reg_value, momentum=0.9)\n",
        "\n",
        "        # Train the model\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                if reg_value is not None:\n",
        "                    l1_reg = reg_value * torch.norm(model.fc.weight, p=1)\n",
        "                    l2_reg = reg_value * torch.norm(model.fc.weight, p=2)\n",
        "                    loss = loss + l1_reg + l2_reg\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                \n",
        "\n",
        "            # Compute accuracy on validation set\n",
        "            model.eval()\n",
        "            valid_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in valid_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    valid_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += targets.size(0)\n",
        "                    correct += (predicted == targets).sum().item()\n",
        "                    \n",
        "            valid_loss /= len(valid_loader)\n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Store accuracy for each fold\n",
        "            if reg_value is None:\n",
        "                fold_none_acc.append(accuracy)\n",
        "            elif reg_name == 'l1':\n",
        "                fold_l1_acc.append(accuracy)\n",
        "            elif reg_name == 'l2':\n",
        "                fold_l2_acc.append(accuracy)\n",
        "\n",
        "            print(f\"Fold {fold_idx+1}: Train loss = {train_loss:.3f}, Validation loss = {valid_loss:.3f}, Validation accuracy = {accuracy:.2f}% for {reg_name} regularization\")\n",
        "#      # Calculate average accuracy for each regularization method\n",
        "\n",
        "if len(fold_none_acc) > 0:\n",
        "    none_avg_acc = sum(fold_none_acc) / len(fold_none_acc)\n",
        "    print(f\"Average validation accuracy with no regularization: {none_avg_acc:.2f}%\")\n",
        "\n",
        "if len(fold_l1_acc) > 0:\n",
        "    l1_avg_acc = sum(fold_l1_acc) / len(fold_l1_acc)\n",
        "    print(f\"Average validation accuracy with L1 regularization (lambda={reg_value}): {l1_avg_acc:.2f}%\")\n",
        "\n",
        "if len(fold_l2_acc) > 0:\n",
        "    l2_avg_acc = sum(fold_l2_acc) / len(fold_l2_acc)\n",
        "    print(f\"Average validation accuracy with L2 regularization (lambda={reg_value}): {l2_avg_acc:.2f}%\")\n",
        "        \n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjyhd9SJXd6K",
        "outputId": "6aee67e2-05d6-4d3c-cd06-2bbb95dfbc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using regularization method: none\n",
            "Using fold 0\n",
            "Fold 1: Train loss = 2896.210, Validation loss = 0.430, Validation accuracy = 85.44% for none regularization\n",
            "Fold 1: Train loss = 1200.666, Validation loss = 0.414, Validation accuracy = 86.07% for none regularization\n",
            "Fold 1: Train loss = 1089.864, Validation loss = 0.417, Validation accuracy = 86.36% for none regularization\n",
            "Fold 1: Train loss = 1033.723, Validation loss = 0.397, Validation accuracy = 86.72% for none regularization\n",
            "Fold 1: Train loss = 1001.844, Validation loss = 0.406, Validation accuracy = 86.45% for none regularization\n",
            "Using fold 1\n",
            "Fold 2: Train loss = 1070.375, Validation loss = 0.353, Validation accuracy = 87.96% for none regularization\n",
            "Fold 2: Train loss = 1012.414, Validation loss = 0.369, Validation accuracy = 87.08% for none regularization\n",
            "Fold 2: Train loss = 985.818, Validation loss = 0.392, Validation accuracy = 87.03% for none regularization\n",
            "Fold 2: Train loss = 959.623, Validation loss = 0.381, Validation accuracy = 86.80% for none regularization\n",
            "Fold 2: Train loss = 947.978, Validation loss = 0.395, Validation accuracy = 86.92% for none regularization\n",
            "Using fold 2\n",
            "Fold 3: Train loss = 1050.738, Validation loss = 0.408, Validation accuracy = 87.32% for none regularization\n",
            "Fold 3: Train loss = 989.937, Validation loss = 0.364, Validation accuracy = 87.15% for none regularization\n",
            "Fold 3: Train loss = 975.371, Validation loss = 0.375, Validation accuracy = 87.62% for none regularization\n",
            "Fold 3: Train loss = 964.221, Validation loss = 0.382, Validation accuracy = 87.20% for none regularization\n",
            "Fold 3: Train loss = 954.918, Validation loss = 0.381, Validation accuracy = 86.94% for none regularization\n",
            "Using fold 3\n",
            "Fold 4: Train loss = 1060.988, Validation loss = 0.360, Validation accuracy = 87.38% for none regularization\n",
            "Fold 4: Train loss = 996.920, Validation loss = 0.384, Validation accuracy = 87.49% for none regularization\n",
            "Fold 4: Train loss = 977.254, Validation loss = 0.352, Validation accuracy = 87.89% for none regularization\n",
            "Fold 4: Train loss = 972.304, Validation loss = 0.366, Validation accuracy = 87.70% for none regularization\n",
            "Fold 4: Train loss = 960.149, Validation loss = 0.382, Validation accuracy = 87.40% for none regularization\n",
            "Using fold 4\n",
            "Fold 5: Train loss = 1083.210, Validation loss = 0.362, Validation accuracy = 87.80% for none regularization\n",
            "Fold 5: Train loss = 1023.326, Validation loss = 0.391, Validation accuracy = 86.55% for none regularization\n",
            "Fold 5: Train loss = 1001.295, Validation loss = 0.398, Validation accuracy = 86.80% for none regularization\n",
            "Fold 5: Train loss = 979.879, Validation loss = 0.463, Validation accuracy = 85.61% for none regularization\n",
            "Fold 5: Train loss = 969.130, Validation loss = 0.393, Validation accuracy = 86.65% for none regularization\n",
            "Using regularization method: l1\n",
            "Using fold 0\n",
            "Fold 1: Train loss = 22532.591, Validation loss = 0.501, Validation accuracy = 83.77% for l1 regularization\n",
            "Fold 1: Train loss = 2655.419, Validation loss = 0.522, Validation accuracy = 82.94% for l1 regularization\n",
            "Fold 1: Train loss = 2368.797, Validation loss = 0.532, Validation accuracy = 82.51% for l1 regularization\n",
            "Fold 1: Train loss = 2329.321, Validation loss = 0.536, Validation accuracy = 82.55% for l1 regularization\n",
            "Fold 1: Train loss = 2303.537, Validation loss = 0.527, Validation accuracy = 82.98% for l1 regularization\n",
            "Using fold 1\n",
            "Fold 2: Train loss = 2391.969, Validation loss = 0.513, Validation accuracy = 82.80% for l1 regularization\n",
            "Fold 2: Train loss = 2295.066, Validation loss = 0.515, Validation accuracy = 82.93% for l1 regularization\n",
            "Fold 2: Train loss = 2289.807, Validation loss = 0.540, Validation accuracy = 81.48% for l1 regularization\n",
            "Fold 2: Train loss = 2280.304, Validation loss = 0.517, Validation accuracy = 82.63% for l1 regularization\n",
            "Fold 2: Train loss = 2285.378, Validation loss = 0.521, Validation accuracy = 82.65% for l1 regularization\n",
            "Using fold 2\n",
            "Fold 3: Train loss = 2356.391, Validation loss = 0.527, Validation accuracy = 82.70% for l1 regularization\n",
            "Fold 3: Train loss = 2265.957, Validation loss = 0.564, Validation accuracy = 82.18% for l1 regularization\n",
            "Fold 3: Train loss = 2256.270, Validation loss = 0.551, Validation accuracy = 82.16% for l1 regularization\n",
            "Fold 3: Train loss = 2252.336, Validation loss = 0.556, Validation accuracy = 81.48% for l1 regularization\n",
            "Fold 3: Train loss = 2248.465, Validation loss = 0.536, Validation accuracy = 82.01% for l1 regularization\n",
            "Using fold 3\n",
            "Fold 4: Train loss = 2341.759, Validation loss = 0.545, Validation accuracy = 82.43% for l1 regularization\n",
            "Fold 4: Train loss = 2259.362, Validation loss = 0.508, Validation accuracy = 83.65% for l1 regularization\n",
            "Fold 4: Train loss = 2252.244, Validation loss = 0.532, Validation accuracy = 82.54% for l1 regularization\n",
            "Fold 4: Train loss = 2245.694, Validation loss = 0.522, Validation accuracy = 82.51% for l1 regularization\n",
            "Fold 4: Train loss = 2247.435, Validation loss = 0.520, Validation accuracy = 82.42% for l1 regularization\n",
            "Using fold 4\n",
            "Fold 5: Train loss = 2334.083, Validation loss = 0.544, Validation accuracy = 82.55% for l1 regularization\n",
            "Fold 5: Train loss = 2242.387, Validation loss = 0.544, Validation accuracy = 82.58% for l1 regularization\n",
            "Fold 5: Train loss = 2234.714, Validation loss = 0.557, Validation accuracy = 82.01% for l1 regularization\n",
            "Fold 5: Train loss = 2235.958, Validation loss = 0.573, Validation accuracy = 81.86% for l1 regularization\n",
            "Fold 5: Train loss = 2231.509, Validation loss = 0.553, Validation accuracy = 81.99% for l1 regularization\n",
            "Using regularization method: l2\n",
            "Using fold 0\n",
            "Fold 1: Train loss = 6151.411, Validation loss = 1.558, Validation accuracy = 55.20% for l2 regularization\n",
            "Fold 1: Train loss = 5806.331, Validation loss = 4.865, Validation accuracy = 23.08% for l2 regularization\n",
            "Fold 1: Train loss = 5623.796, Validation loss = 1.420, Validation accuracy = 58.46% for l2 regularization\n",
            "Fold 1: Train loss = 5576.626, Validation loss = 1.610, Validation accuracy = 52.43% for l2 regularization\n",
            "Fold 1: Train loss = 5583.956, Validation loss = 1.561, Validation accuracy = 55.17% for l2 regularization\n",
            "Using fold 1\n",
            "Fold 2: Train loss = 6923.401, Validation loss = 1.595, Validation accuracy = 55.29% for l2 regularization\n",
            "Fold 2: Train loss = 5769.076, Validation loss = 1.649, Validation accuracy = 52.64% for l2 regularization\n",
            "Fold 2: Train loss = 5750.609, Validation loss = 1.493, Validation accuracy = 55.86% for l2 regularization\n",
            "Fold 2: Train loss = 5735.286, Validation loss = 1.452, Validation accuracy = 57.30% for l2 regularization\n",
            "Fold 2: Train loss = 5648.929, Validation loss = 1.519, Validation accuracy = 55.05% for l2 regularization\n",
            "Using fold 2\n",
            "Fold 3: Train loss = 5788.573, Validation loss = 2.529, Validation accuracy = 40.34% for l2 regularization\n",
            "Fold 3: Train loss = 5643.585, Validation loss = 1.596, Validation accuracy = 53.51% for l2 regularization\n",
            "Fold 3: Train loss = 5691.487, Validation loss = 1.435, Validation accuracy = 57.97% for l2 regularization\n",
            "Fold 3: Train loss = 5449.351, Validation loss = 1.348, Validation accuracy = 61.73% for l2 regularization\n",
            "Fold 3: Train loss = 5018.279, Validation loss = 2.596, Validation accuracy = 48.43% for l2 regularization\n",
            "Using fold 3\n",
            "Fold 4: Train loss = 5247.858, Validation loss = 1.193, Validation accuracy = 65.87% for l2 regularization\n",
            "Fold 4: Train loss = 4991.411, Validation loss = 1.287, Validation accuracy = 62.82% for l2 regularization\n",
            "Fold 4: Train loss = 4947.551, Validation loss = 1.271, Validation accuracy = 62.76% for l2 regularization\n",
            "Fold 4: Train loss = 4937.386, Validation loss = 1.199, Validation accuracy = 64.47% for l2 regularization\n",
            "Fold 4: Train loss = 4992.491, Validation loss = 1.067, Validation accuracy = 68.93% for l2 regularization\n",
            "Using fold 4\n",
            "Fold 5: Train loss = 5164.572, Validation loss = 1.308, Validation accuracy = 61.54% for l2 regularization\n",
            "Fold 5: Train loss = 4904.759, Validation loss = 1.387, Validation accuracy = 63.36% for l2 regularization\n",
            "Fold 5: Train loss = 4917.146, Validation loss = 1.160, Validation accuracy = 65.24% for l2 regularization\n",
            "Fold 5: Train loss = 4927.114, Validation loss = 1.223, Validation accuracy = 63.72% for l2 regularization\n",
            "Fold 5: Train loss = 4940.492, Validation loss = 1.098, Validation accuracy = 67.04% for l2 regularization\n",
            "Average validation accuracy with L2 regularization (lambda=0.0001): 57.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with dropout"
      ],
      "metadata": {
        "id": "V3eREj6GZ5y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with dropout\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(p=0.2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "        self.bn_fc = nn.BatchNorm1d(47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        x = self.bn_fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 32\n",
        "lr = 0.01\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Training with RMSprop optimizer and CosineAnnealingLR learning rate scheduler\")\n",
        "\n",
        "# Apply k-fold cross-validation for dropout\n",
        "accuracies = []\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
        "    print(f\"Fold [{fold+1}/{kf.n_splits}]\")\n",
        "\n",
        "\n",
        "    # Initialize the model, optimizer, and scheduler for the current fold\n",
        "    model = CNN().to(device)\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    num_batches = len(train_loader) * num_epochs\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_batches)\n",
        "\n",
        "    # Define the data loaders for the current fold\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(train_data, batch_size=batch_size, sampler=val_sampler)\n",
        "    \n",
        "    # Add dropout to the model\n",
        "    model.drop1 = nn.Dropout(p=0.2)\n",
        "    model.drop2 = nn.Dropout(p=0.2)\n",
        "    \n",
        "    # Train the model for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Accuracy: {accuracy:.2f}%\")\n",
        "            \n",
        "# Evaluate the model on the test set for the current fold\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Fold [{fold+1}/{kf.n_splits}] Test Accuracy: {accuracy:.2f}%\")\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the average test accuracy over all folds\n",
        "    average_accuracy = sum(accuracies) / len(accuracies)\n",
        "print(f\"Average Test Accuracy: {average_accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivu2VNFFqga3",
        "outputId": "e15ccf4d-f1d0-4742-aaee-47a3d1118af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with RMSprop optimizer and CosineAnnealingLR learning rate scheduler\n",
            "Fold [1/5]\n",
            "Fold [2/5]\n",
            "Fold [3/5]\n",
            "Fold [4/5]\n",
            "Fold [5/5]\n",
            "Epoch [5/5] Validation Accuracy: 88.31%\n",
            "Fold [5/5] Test Accuracy: 87.74%\n",
            "Average Test Accuracy: 87.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DyrNni30QOsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "without dropout"
      ],
      "metadata": {
        "id": "8P6KIrWtZ-h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#without dropout\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 32\n",
        "lr = 0.01\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Training with RMSprop optimizer and CosineAnnealingLR learning rate scheduler witout dropout\")\n",
        "\n",
        "# Apply k-fold cross-validation without dropout\n",
        "accuracies = []\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
        "    print(f\"Fold [{fold+1}/{kf.n_splits}]\")\n",
        "\n",
        "    # Initialize the model, optimizer, and scheduler for the current fold\n",
        "    model = CNN().to(device)\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    num_batches = len(train_loader) * num_epochs\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_batches)\n",
        "\n",
        "    # Define the data loaders for the current fold\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(train_data, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Train the model for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        # Evaluate the model on the test set for the current fold\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            print(f\"Fold [{fold+1}/{kf.n_splits}] Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "print(f\"Overall Test Accuracy: {sum(accuracies)/len(accuracies):.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbr4bnQ_xMW-",
        "outputId": "896d7420-bd2d-4a3f-ed99-765b8161c810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with RMSprop optimizer and CosineAnnealingLR learning rate scheduler witout dropout\n",
            "Fold [1/5]\n",
            "Epoch [1/5] Validation Accuracy: 84.31%\n",
            "Fold [1/5] Test Accuracy: 84.04%\n",
            "Epoch [2/5] Validation Accuracy: 86.94%\n",
            "Fold [1/5] Test Accuracy: 86.30%\n",
            "Epoch [3/5] Validation Accuracy: 87.41%\n",
            "Fold [1/5] Test Accuracy: 86.98%\n",
            "Epoch [4/5] Validation Accuracy: 88.01%\n",
            "Fold [1/5] Test Accuracy: 87.45%\n",
            "Epoch [5/5] Validation Accuracy: 88.15%\n",
            "Fold [1/5] Test Accuracy: 87.56%\n",
            "Fold [2/5]\n",
            "Epoch [1/5] Validation Accuracy: 85.45%\n",
            "Fold [2/5] Test Accuracy: 85.08%\n",
            "Epoch [2/5] Validation Accuracy: 86.45%\n",
            "Fold [2/5] Test Accuracy: 86.57%\n",
            "Epoch [3/5] Validation Accuracy: 87.04%\n",
            "Fold [2/5] Test Accuracy: 86.68%\n",
            "Epoch [4/5] Validation Accuracy: 87.69%\n",
            "Fold [2/5] Test Accuracy: 87.52%\n",
            "Epoch [5/5] Validation Accuracy: 87.78%\n",
            "Fold [2/5] Test Accuracy: 87.65%\n",
            "Fold [3/5]\n",
            "Epoch [1/5] Validation Accuracy: 84.56%\n",
            "Fold [3/5] Test Accuracy: 84.19%\n",
            "Epoch [2/5] Validation Accuracy: 86.31%\n",
            "Fold [3/5] Test Accuracy: 86.32%\n",
            "Epoch [3/5] Validation Accuracy: 87.01%\n",
            "Fold [3/5] Test Accuracy: 87.10%\n",
            "Epoch [4/5] Validation Accuracy: 87.70%\n",
            "Fold [3/5] Test Accuracy: 87.65%\n",
            "Epoch [5/5] Validation Accuracy: 87.72%\n",
            "Fold [3/5] Test Accuracy: 87.72%\n",
            "Fold [4/5]\n",
            "Epoch [1/5] Validation Accuracy: 85.05%\n",
            "Fold [4/5] Test Accuracy: 84.59%\n",
            "Epoch [2/5] Validation Accuracy: 86.35%\n",
            "Fold [4/5] Test Accuracy: 86.29%\n",
            "Epoch [3/5] Validation Accuracy: 86.74%\n",
            "Fold [4/5] Test Accuracy: 86.93%\n",
            "Epoch [4/5] Validation Accuracy: 87.54%\n",
            "Fold [4/5] Test Accuracy: 87.78%\n",
            "Epoch [5/5] Validation Accuracy: 87.85%\n",
            "Fold [4/5] Test Accuracy: 87.80%\n",
            "Fold [5/5]\n",
            "Epoch [1/5] Validation Accuracy: 86.17%\n",
            "Fold [5/5] Test Accuracy: 85.40%\n",
            "Epoch [2/5] Validation Accuracy: 86.75%\n",
            "Fold [5/5] Test Accuracy: 86.57%\n",
            "Epoch [3/5] Validation Accuracy: 87.87%\n",
            "Fold [5/5] Test Accuracy: 87.52%\n",
            "Epoch [4/5] Validation Accuracy: 88.12%\n",
            "Fold [5/5] Test Accuracy: 87.61%\n",
            "Epoch [5/5] Validation Accuracy: 88.20%\n",
            "Fold [5/5] Test Accuracy: 87.78%\n",
            "Overall Test Accuracy: 86.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIND THE BEST NUMBER OF NEURONS"
      ],
      "metadata": {
        "id": "dFIIM00TaigX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define the device to be used\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, hidden_neurons):\n",
        "        super(CNN, self).__init__()\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(p=0.2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(64*7*7, self.hidden_neurons)\n",
        "        self.bn_fc = nn.BatchNorm1d(self.hidden_neurons)\n",
        "        self.output = nn.Linear(self.hidden_neurons, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        x = self.bn_fc(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Set up the cross-validation\n",
        "k = 5\n",
        "num_epochs=5\n",
        "hidden_neurons_list = [34, 64, 128, 512, 1024]\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and evaluate the model for each value of hidden_neurons\n",
        "best_hidden_neurons = None\n",
        "best_accuracy = 0.0\n",
        "for hidden_neurons in hidden_neurons_list:\n",
        "    accuracy_list = []\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_data)):\n",
        "        # Split the data into training and validation sets for this fold\n",
        "        train_subset = torch.utils.data.Subset(train_data, train_index)\n",
        "        val_subset = torch.utils.data.Subset(train_data, val_index)\n",
        "        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # Initialize the model, optimizer, and scheduler for the current fold and hidden neuron value\n",
        "        model = CNN(hidden_neurons)\n",
        "        model.to(device)\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Train the model for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Adjust the learning rate using the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate the model on the validation set for this fold\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Compute the average training and validation accuracy for this fold and epoch\n",
        "        train_accuracy = train_correct / len(train_subset)\n",
        "        val_accuracy = val_correct / len(val_subset)\n",
        "        accuracy_list.append(val_accuracy)\n",
        "\n",
        "# Compute the average validation accuracy across all folds for this hidden neuron value\n",
        "avg_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "\n",
        "# Update the best hidden neuron value and accuracy if this value is better than the previous best\n",
        "if avg_accuracy > best_accuracy:\n",
        "    best_hidden_neurons = hidden_neurons\n",
        "    best_accuracy = avg_accuracy\n",
        "print(f\"Best number of hidden neurons: {best_hidden_neurons}\")\n",
        "print(f\"Best validation accuracy on the validation set: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "cPNx2xpIF_uN",
        "outputId": "a04fccb5-3180-49ea-9cfd-b66ee5ef2ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Best number of hidden neurons: 1024\n",
            "Best validation accuracy on the validation set: 0.8374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wI89nheOWnWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('GPU is available')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('GPU is not available, using CPU instead')\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_hidden_layers, hidden_neurons):\n",
        "        super(CNN, self).__init__()\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(p=0.2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(p=0.2)\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(num_hidden_layers):\n",
        "            if i == 0:\n",
        "                input_size = 64 * 7 * 7\n",
        "            else:\n",
        "                input_size = hidden_neurons\n",
        "            self.hidden_layers.append(nn.Linear(input_size, hidden_neurons))\n",
        "            self.hidden_layers.append(nn.BatchNorm1d(hidden_neurons))\n",
        "            self.hidden_layers.append(nn.ReLU())\n",
        "            self.hidden_layers.append(nn.Dropout(p=0.2))\n",
        "        self.output = nn.Linear(hidden_neurons, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Define range of values for number of hidden layers\n",
        "hidden_layer_sizes = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Set up k-fold cross-validation\n",
        "k = 5\n",
        "num_epochs = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "hidden_neurons = 1024\n",
        "\n",
        "# Initialize lists to record results\n",
        "accuracies = []\n",
        "best_num_hidden_layers = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "for num_hidden_layers in hidden_layer_sizes:\n",
        "    fold_accuracies = []\n",
        "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_data)):\n",
        "        # Split data into train and validation sets\n",
        "        train_subset = torch.utils.data.Subset(train_data, train_indices)\n",
        "        val_subset = torch.utils.data.Subset(train_data, val_indices)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "        # Initialize the model, optimizer, and scheduler for the current fold and hidden neuron value\n",
        "        model = CNN(num_hidden_layers, hidden_neurons).to(device)\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "            # Train the model\n",
        "        # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Compute validation accuracy\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        fold_accuracies.append(accuracy)\n",
        "\n",
        "# Compute average validation accuracy for this number of hidden layers\n",
        "avg_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
        "accuracies.append(avg_accuracy)\n",
        "\n",
        "# Check if this is the best accuracy so far\n",
        "if avg_accuracy > best_accuracy:\n",
        "    best_accuracy = avg_accuracy\n",
        "    best_num_hidden_layers = num_hidden_layers\n",
        "\n",
        "\n",
        "print(f\"Best number of hidden layers: {best_num_hidden_layers}\")\n",
        "print(f\"Best validation accuracy: {best_accuracy}\")\n",
        "print(f\"Time taken: {time.time()-start_time} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys5DhIY7GTmT",
        "outputId": "f239204c-1d88-4b03-f9c5-9c3822670c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n",
            "Best number of hidden layers: 5\n",
            "Best validation accuracy: 0.8356294326241134\n",
            "Time taken: 900.2361905574799 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_bqKUXuWqgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn.metrics\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('GPU is available')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('GPU is not available, using CPU instead')\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_hidden_layers, hidden_neurons):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(p=0.2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(p=0.2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.drop3 = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.drop4 = nn.Dropout(p=0.2)\n",
        "        self.fc3 = nn.Linear(1024, 1024)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.drop5 = nn.Dropout(p=0.2)\n",
        "        self.fc4 = nn.Linear(1024, 1024)\n",
        "        self.bn6 = nn.BatchNorm1d(1024)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.drop6 = nn.Dropout(p=0.2)\n",
        "        self.fc5 = nn.Linear(1024, 47)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.drop3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.drop4(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.drop5(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn6(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.drop6(x)\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Set up k-fold cross-validation\n",
        "k = 5\n",
        "num_epochs = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize the model, optimizer, and scheduler for the current fold and hidden neuron value\n",
        "# Initialize the model, optimizer, and scheduler for the current fold and hidden neuron value\n",
        "model = CNN(num_hidden_layers=3, hidden_neurons=1024).to(device)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "# Set up k-fold cross-validation\n",
        "k = 5\n",
        "num_epochs = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy and time for each fold\n",
        "accuracy_list = []\n",
        "time_list = []\n",
        "\n",
        "# Initialize confusion matrix, precision, recall, and f1 score lists\n",
        "cm_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_indices, test_indices) in enumerate(kf.split(train_data)):\n",
        "    print(f'Fold {fold+1}:')\n",
        "\n",
        "    # Initialize the model, optimizer, and scheduler for the current fold\n",
        "    # model = CNN().to(device)\n",
        "    model = CNN(num_hidden_layers=5, hidden_neurons=1024).to(device)\n",
        "\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize lists to store training and validation losses\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "\n",
        "    # Train the model for the current fold\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_loss_list.append(train_loss)\n",
        "        \n",
        "\n",
        "\n",
        "        # Test the model on the validation set\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in train_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                valid_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        valid_loss /= len(train_loader)\n",
        "        valid_loss_list.append(valid_loss)\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    # Test the model on the test set for the current fold\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predicted_labels += predicted.cpu().tolist()\n",
        "        true_labels += labels.cpu().tolist()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Calculate accuracy and time for the current fold\n",
        "accuracy = 100.0 * correct / total\n",
        "time_taken = time.time() - start_time\n",
        "# Calculate confusion matrix, precision, recall, and f1 score for the current fold\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "print(f\"Fold {fold}: Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}, Time taken: {time_taken:.2f} seconds\")\n",
        "print(f\"Fold {fold}: Training Loss: {train_loss_list[-1]:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}, Time taken: {time_taken:.2f} seconds\")\n",
        "\n",
        "print(f\"Time taken: {time.time()-start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "qt0ATiW2LyeM",
        "outputId": "de1fcfe4-381f-49cf-fe91-d3e0e5e9c833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n",
            "Fold 1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-500210eeb7f5>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_num_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#witout batch normalize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
        "test_data = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the CNN architectures\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc = nn.Linear(64*7*7, 47)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 64*7*7)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 32\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Training without Batch Normalization\")\n",
        "\n",
        "# Apply k-fold cross-validation\n",
        "accuracies = []\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
        "    print(f\"Fold [{fold+1}/{kf.n_splits}]\")\n",
        "\n",
        "    # Initialize the model, optimizer, and scheduler for the current fold\n",
        "    model = CNN().to(device)\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define the data loaders for the current fold\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(train_data, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Train the model for the current fold\n",
        "    for epoch in range(epochs):\n",
        "        # Train for one epoch\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate the model for one epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "            \n",
        "            # Print the loss and accuracy for the current epoch\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            val_acc = 100 * correct / total\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    # Calculate the accuracy for the current fold and add it to the list of accuracies\n",
        "    fold_acc = 100 * correct / total\n",
        "    accuracies.append(fold_acc)\n",
        "\n",
        "# Calculate the average accuracy over all folds\n",
        "avg_acc = sum(accuracies) / len(accuracies)\n",
        "print(f\"Average Accuracy: {avg_acc:.2f}%\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sF_ReMwF5eJ",
        "outputId": "ae1c1b95-c0cb-4c29-e979-5dd575ede4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 561753746/561753746 [00:14<00:00, 38950758.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n",
            "Training without Batch Normalization\n",
            "Fold [1/5]\n",
            "Epoch [1/10] Val Loss: 0.0959, Val Acc: 84.44%\n",
            "Epoch [2/10] Val Loss: 0.0863, Val Acc: 85.62%\n",
            "Epoch [3/10] Val Loss: 0.0798, Val Acc: 86.65%\n",
            "Epoch [4/10] Val Loss: 0.0812, Val Acc: 86.09%\n",
            "Epoch [5/10] Val Loss: 0.0795, Val Acc: 86.96%\n",
            "Epoch [6/10] Val Loss: 0.0783, Val Acc: 86.84%\n",
            "Epoch [7/10] Val Loss: 0.0775, Val Acc: 86.80%\n",
            "Epoch [8/10] Val Loss: 0.0781, Val Acc: 86.86%\n",
            "Epoch [9/10] Val Loss: 0.0791, Val Acc: 87.19%\n",
            "Epoch [10/10] Val Loss: 0.0795, Val Acc: 86.78%\n",
            "Fold [2/5]\n",
            "Epoch [1/10] Val Loss: 0.0972, Val Acc: 83.96%\n",
            "Epoch [2/10] Val Loss: 0.0866, Val Acc: 85.30%\n",
            "Epoch [3/10] Val Loss: 0.0834, Val Acc: 85.73%\n",
            "Epoch [4/10] Val Loss: 0.0789, Val Acc: 86.69%\n",
            "Epoch [5/10] Val Loss: 0.0777, Val Acc: 86.94%\n",
            "Epoch [6/10] Val Loss: 0.0793, Val Acc: 86.62%\n",
            "Epoch [7/10] Val Loss: 0.0799, Val Acc: 86.59%\n",
            "Epoch [8/10] Val Loss: 0.0801, Val Acc: 86.93%\n",
            "Epoch [9/10] Val Loss: 0.0779, Val Acc: 86.94%\n",
            "Epoch [10/10] Val Loss: 0.0792, Val Acc: 87.20%\n",
            "Fold [3/5]\n",
            "Epoch [1/10] Val Loss: 0.0962, Val Acc: 84.36%\n",
            "Epoch [2/10] Val Loss: 0.0880, Val Acc: 85.28%\n",
            "Epoch [3/10] Val Loss: 0.0835, Val Acc: 86.14%\n",
            "Epoch [4/10] Val Loss: 0.0821, Val Acc: 86.26%\n",
            "Epoch [5/10] Val Loss: 0.0804, Val Acc: 86.54%\n",
            "Epoch [6/10] Val Loss: 0.0802, Val Acc: 86.84%\n",
            "Epoch [7/10] Val Loss: 0.0805, Val Acc: 86.48%\n",
            "Epoch [8/10] Val Loss: 0.0809, Val Acc: 86.67%\n",
            "Epoch [9/10] Val Loss: 0.0806, Val Acc: 86.75%\n",
            "Epoch [10/10] Val Loss: 0.0790, Val Acc: 86.64%\n",
            "Fold [4/5]\n",
            "Epoch [1/10] Val Loss: 0.0912, Val Acc: 84.38%\n",
            "Epoch [2/10] Val Loss: 0.0842, Val Acc: 86.03%\n",
            "Epoch [3/10] Val Loss: 0.0835, Val Acc: 85.96%\n",
            "Epoch [4/10] Val Loss: 0.0817, Val Acc: 86.85%\n",
            "Epoch [5/10] Val Loss: 0.0800, Val Acc: 87.13%\n",
            "Epoch [6/10] Val Loss: 0.0779, Val Acc: 87.23%\n",
            "Epoch [7/10] Val Loss: 0.0808, Val Acc: 86.95%\n",
            "Epoch [8/10] Val Loss: 0.0805, Val Acc: 87.23%\n",
            "Epoch [9/10] Val Loss: 0.0819, Val Acc: 86.99%\n",
            "Epoch [10/10] Val Loss: 0.0808, Val Acc: 87.12%\n",
            "Fold [5/5]\n",
            "Epoch [1/10] Val Loss: 0.0920, Val Acc: 84.95%\n",
            "Epoch [2/10] Val Loss: 0.0884, Val Acc: 85.34%\n",
            "Epoch [3/10] Val Loss: 0.0790, Val Acc: 86.88%\n",
            "Epoch [4/10] Val Loss: 0.0800, Val Acc: 86.75%\n",
            "Epoch [5/10] Val Loss: 0.0778, Val Acc: 87.04%\n",
            "Epoch [6/10] Val Loss: 0.0769, Val Acc: 87.33%\n",
            "Epoch [7/10] Val Loss: 0.0795, Val Acc: 87.05%\n",
            "Epoch [8/10] Val Loss: 0.0797, Val Acc: 86.80%\n",
            "Epoch [9/10] Val Loss: 0.0791, Val Acc: 86.93%\n",
            "Epoch [10/10] Val Loss: 0.0807, Val Acc: 86.76%\n",
            "Average Accuracy: 86.90%\n",
            "Execution time: 1884.63 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib._api.deprecation import MatplotlibDeprecationWarning\n",
        "import matplotlib\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn.metrics\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('GPU is available')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('GPU is not available, using CPU instead')\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_hidden_layers, hidden_neurons):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop1 = nn.Dropout(p=0.2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.drop2 = nn.Dropout(p=0.2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.drop3 = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.drop4 = nn.Dropout(p=0.2)\n",
        "        self.fc3 = nn.Linear(1024, 1024)\n",
        "        self.bn5 = nn.BatchNorm1d(1024)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.drop5 = nn.Dropout(p=0.2)\n",
        "        self.fc4 = nn.Linear(1024, 1024)\n",
        "        self.bn6 = nn.BatchNorm1d(1024)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.drop6 = nn.Dropout(p=0.2)\n",
        "        self.fc5 = nn.Linear(1024, 47)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.drop3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.drop4(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.drop5(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn6(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.drop6(x)\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, optimizer, and scheduler for the current fold and hidden neuron value\n",
        "model = CNN(num_hidden_layers=5, hidden_neurons=1024).to(device)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "# Set up k-fold cross-validation\n",
        "k = 5\n",
        "num_epochs = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy and time for each fold\n",
        "accuracy_list = []\n",
        "time_list = []\n",
        "\n",
        "# Initialize confusion matrix, precision, recall, and f1 score lists\n",
        "cm_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "train_loss_lists = []\n",
        "valid_loss_lists = []\n",
        "\n",
        "# Loop through each fold\n",
        "for fold, (train_indices, test_indices) in enumerate(kf.split(train_data)):\n",
        "    print(f'Fold {fold+1}:')\n",
        "    train_subset = torch.utils.data.Subset(train_data, train_indices)\n",
        "    val_subset = torch.utils.data.Subset(train_data, test_indices)  \n",
        "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Initialize the model, optimizer, and scheduler for the current fold\n",
        "    model = CNN(num_hidden_layers=5, hidden_neurons=1024).to(device)\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_loss_lists.append(train_loss_list)\n",
        "    valid_loss_lists.append(valid_loss_list)\n",
        "\n",
        "\n",
        "    # Initialize lists to store training and validation losses and accuracy\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    accuracy_list = []\n",
        "    test_loss_list=[]\n",
        "\n",
        "    # Train the model for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "        # Test the model on the validation set\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                valid_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        valid_loss /= len(val_loader)\n",
        "        valid_loss_list.append(valid_loss)\n",
        "        \n",
        "        accuracy = correct / total\n",
        "        accuracy_list.append(accuracy)\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "        \n",
        "\n",
        "# Test the model on the test set for the current fold\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predicted_labels += predicted.cpu().tolist()\n",
        "        true_labels += labels.cpu().tolist()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    test_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    test_loss /= len(test_loader)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_accuracy = correct / total\n",
        "\n",
        "# Calculate accuracy and time for the current fold\n",
        "accuracy = 100.0 * correct / total\n",
        "time_taken = time.time() - start_time\n",
        "accuracy_list.append(accuracy)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "epochs = np.arange(num_epochs)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i, (train_loss_list, valid_loss_list) in enumerate(zip(train_loss_lists, valid_loss_lists)):\n",
        "    plt.plot(epochs, train_loss_list, label=f'Train loss Fold {i+1}')\n",
        "    plt.plot(epochs, valid_loss_list, label=f'Valid loss Fold {i+1}')\n",
        "\n",
        "plt.title('Loss Function')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Plot loss function and accuracy graphs\n",
        "epochs = np.arange(len(accuracy_list))  # Change this line\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(epochs, train_loss_list, label='Train loss')\n",
        "plt.plot(epochs, valid_loss_list, label='Valid loss')\n",
        "plt.title('Loss Function')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Calculate and print confusion matrix, F1 score, accuracy, precision, and recall\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(cm)\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 score: {f1:.4f}')\n",
        "# Print the results for the current fold\n",
        "print(f'Train loss: {train_loss_list[-1]:.4f}')\n",
        "print(f'Validation loss: {valid_loss_list[-1]:.4f}')\n",
        "print(f'Test loss: {test_loss_list[-1]:.4f}')\n",
        "print(f'Validation accuracy: {accuracy_list[-1]*100:.2f}%')\n",
        "print(f'Test accuracy: {test_accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CUEfc7RrW40P",
        "outputId": "fa6f106e-bac7-4efd-b6ef-50420112b3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n",
            "Fold 1:\n",
            "Fold 2:\n",
            "Fold 3:\n",
            "Fold 4:\n",
            "Fold 5:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0ca2cd0c4c12>\u001b[0m in \u001b[0;36m<cell line: 242>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Change this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss Function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (6,) and (5,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}